@article{Efron1987,
abstract = {We consider the problem of setting approximate confidence intervals for a single parameter $\theta$ in a multiparameter family. The standard approximate intervals based on maximum likelihood theory, $\theta$ $\sigma$z($\alpha$), can be quite misleading. In practice, tricks based on transformations, bias corrections, and so forth, are often used to improve their accuracy. The bootstrap confidence intervals discussed in this article automatically incorporate such tricks without requiring the statistician to think them through for each new application, at the price of a considerable increase in computational effort. The new intervals incorporate an improvement over previously suggested methods, which results in second-order correctness in a wide variety of problems. In addition to parametric families, bootstrap intervals are also developed for nonparametric situations.},
author = {Efron, Bradley},
doi = {10.2307/2289144},
file = {:home/hawk31/ownCloud/Master/FBIO/Trabajo{\_}fin{\_}asignatura/BCa.pdf:pdf},
isbn = {01621459},
issn = {01621459},
journal = {Journal of the American statistical Association},
number = {397},
pages = {171--185},
pmid = {138},
title = {{Better bootstrap confidence intervals}},
url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1987.10478410},
volume = {82},
year = {1987}
}
@article{EnricoBonatesta2015,
author = {{Enrico Bonatesta}, Christoph Horejs-Kainrath},
file = {:home/hawk31/ownCloud/Master/FBIO/Trabajo{\_}fin{\_}asignatura/msa.pdf:pdf},
journal = {Bioconductor software manual},
pages = {1--59},
title = {{msa: An R Package for Multiple Sequence Alignment}},
year = {2015}
}
@article{Jehl2015,
abstract = {BACKGROUND:Multiple sequence alignments (MSA) are widely used in sequence analysis for a variety of tasks. Outlier sequences can make downstream analyses unreliable or make the alignments less accurate while they are being constructed. This paper describes a simple method for automatically detecting outliers and accompanying software called OD-seq. It is based on finding sequences whose average distance to the rest of the sequences in a dataset, is anomalous.RESULTS:The software can take a MSA, distance matrix or set of unaligned sequences as input. Outlier sequences are found by examining the average distance of each sequence to the rest. Anomalous average distances are then found using the interquartile range of the distribution of average distances or by bootstrapping them. The complexity of any analysis of a distance matrix is normally at least O(N2                    ) for N sequences. This is prohibitive for large N but is reduced here by using the mBed algorithm from Clustal Omega. This reduces the complexity to O(N log(N)) which makes even very large alignments easy to analyse on a single core. We tested the ability of OD-seq to detect outliers using artificial test cases of sequences from Pfam families, seeded with sequences from other Pfam families. Using a MSA as input, OD-seq is able to detect outliers with very high sensitivity and specificity.CONCLUSION:OD-seq is a practical and simple method to detect outliers in MSAs. It can also detect outliers in sets of unaligned sequences, but with reduced accuracy. For medium sized alignments, of a few thousand sequences, it can detect outliers in a few seconds. Software available as http://www.bioinf.ucd.ie/download/od-seq.tar.gz.},
author = {Jehl, Peter and Sievers, Fabian and Higgins, Desmond G.},
doi = {10.1186/s12859-015-0702-1},
file = {:home/hawk31/ownCloud/Master/FBIO/Trabajo{\_}fin{\_}asignatura/od-seq.pdf:pdf},
issn = {1471-2105},
journal = {BMC Bioinformatics},
keywords = {Outlier,Multiple sequence alignment,multiple sequence alignment,outlier},
number = {1},
pages = {269},
publisher = {BMC Bioinformatics},
title = {{OD-seq: outlier detection in multiple sequence alignments}},
url = {http://www.biomedcentral.com/1471-2105/16/269},
volume = {16},
year = {2015}
}
@article{Learning,
author = {Learning, Probabilistic},
file = {:home/hawk31/ownCloud/Master/FBIO/Trabajo{\_}fin{\_}asignatura/EMnotes.pdf:pdf},
pages = {1--4},
title = {{The EM Algorithm for Gaussian Mixtures The EM Algorithm for Gaussian Mixture Models}},
url = {http://www.ics.uci.edu/{~}smyth/courses/cs274/notes/EMnotes.pdf}
}
@article{Rasmussen2000,
abstract = {In a Bayesian mixture model it is not necessary a priori to limit the number of components to be finite. In this paper an infinite Gaussian mixture model is presented which neatly sidesteps the difficult problem of find- ing the “right” number of mixture components. Inference in the model is done using an efficient parameter-free Markov Chain that relies entirely on Gibbs sampling.},
author = {Rasmussen, Carl E.},
file = {:home/hawk31/ownCloud/Master/FBIO/Trabajo{\_}fin{\_}asignatura/infinitemixture.pdf:pdf},
isbn = {0262194503},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems 12},
pages = {554--560},
title = {{The Infinite Gaussian Mixture Model}},
url = {http://papers.nips.cc/paper/1745-the-infinite-gaussian-mixture-model.pdf},
year = {2000}
}
@article{Sievers2014,
author = {Sievers, F. and Wilm, A. and Dineen, D. and Gibson, T. J. and Karplus, K. and Li, W. and Lopez, R. and McWilliam, H. and Remmert, M. and Soding, J. and Thompson, J. D. and Higgins, D. G.},
doi = {10.1038/msb.2011.75},
file = {:home/hawk31/ownCloud/Master/FBIO/Trabajo{\_}fin{\_}asignatura/clustalomega.pdf:pdf},
issn = {1744-4292},
journal = {Molecular Systems Biology},
keywords = {bioinformatics,hidden markov models,multiple sequence alignment},
number = {1},
pages = {539--539},
title = {{Fast, scalable generation of high-quality protein multiple sequence alignments using Clustal Omega}},
url = {http://msb.embopress.org/cgi/doi/10.1038/msb.2011.75},
volume = {7},
year = {2014}
}
@book{Bishop2006,
abstract = {The dramatic growth in practical applications for machine learning over the last ten years has been accompanied by many important developments in the underlying algorithms and techniques. For example, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic techniques. The practical applicability of Bayesian methods has been greatly enhanced by the development of a range of approximate inference algorithms such as variational Bayes and expectation propagation, while new models based on kernels have had a significant impact on both algorithms and applications. This completely new textbook reflects these recent developments while providing a comprehensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first-year PhD students, as well as researchers and practitioners. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory. The book is suitable for courses on machine learning, statistics, computer science, signal processing, computer vision, data mining, and bioinformatics. Extensive support is provided for course instructors, including more than 400 exercises, graded according to difficulty. Example solutions for a subset of the exercises are available from the book web site, while solutions for the remainder can be obtained by instructors from the publisher. The book is supported by a great deal of additional material, and the reader is encouraged to visit the book web site for the latest information. A forthcoming companion volume will deal with practical aspects of pattern recognition and machine learning, and will include free software implementations of the key algorithms along with example data sets and demonstration programs. Christopher Bishop is Assistant Director at Microsoft Research Cambridge, and also holds a Chair in Computer Science at the University of Edinburgh. He is a Fellow of Darwin College Cambridge, and was recently elected Fellow of the Royal Academy of Engineering. The author's previous textbook "Neural Networks for Pattern Recognition" has been widely adopted.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {Bishop, Christopher M.},
booktitle = {Springer},
doi = {10.1117/1.2819119},
eprint = {0-387-31073-8},
file = {:home/hawk31/ownCloud/Master/FBIO/Trabajo{\_}fin{\_}asignatura/Bishop - Pattern Recognition and Machine Learning.pdf:pdf},
isbn = {9780387310732},
issn = {10179909},
number = {4},
pages = {738},
pmid = {8943268},
title = {{Pattern Recognition and Machine Learning}},
url = {http://www.library.wisc.edu/selectedtocs/bg0137.pdf},
volume = {4},
year = {2006}
}
@article{Huang2013,
abstract = {This paper is concerned with an important issue in finite mixture modelling, the selection of the number of mixing components. We propose a new penalized likelihood method for model selection of finite multivariate Gaussian mixture models. The proposed method is shown to be statistically consistent in determining of the number of components. A modified EM algorithm is developed to simultaneously select the number of components and to estimate the mixing weights, i.e. the mixing probabilities, and unknown parameters of Gaussian distributions. Simulations and a real data analysis are presented to illustrate the performance of the proposed method.},
archivePrefix = {arXiv},
arxivId = {1301.3558},
author = {Huang, Tao and Peng, Heng and Zhang, Kun},
eprint = {1301.3558},
file = {:home/hawk31/ownCloud/Master/FBIO/Trabajo{\_}fin{\_}asignatura/bicmixture.pdf:pdf},
keywords = {em algorithm,gaussian mixture models,model selection,penalized likelihood},
pages = {1--27},
title = {{Model Selection for Gaussian Mixture Models}},
url = {http://arxiv.org/abs/1301.3558},
year = {2013}
}
@article{Gorur2010,
abstract = {In the Bayesian mixture modeling framework it is possible to infer the necessary number of components to model the data and therefore it is unnecessary to explicitly restrict the number of components. Nonparametric mixture models$\backslash$r$\backslash$nsidestep the problem of finding the "correct" number of mixture components by assuming infinitely many components. In this paper Dirichlet process mixture (DPM) models are cast as infinite mixture models and inference using Markov chain Monte Carlo is described. The specification of the priors on the model parameters is often guided by mathematical and practical convenience. The primary goal of this paper is to compare the choice of conjugate and non-conjugate base distributions on a particular class of DPM models which is widely used in applications, the Dirichlet process Gaussian mixture model (DPGMM). We compare computational efficiency and modeling performance of DPGMM defined using a conjugate and a conditionally conjugate base distribution. We show that better density models can result from using a wider class of priors with no or only a modest increase in computational effort.},
author = {G{\"{o}}r{\"{u}}r, Dilan and Rasmussen, Carl Edward},
doi = {10.1007/s11390-010-1051-1},
file = {:home/hawk31/ownCloud/Master/FBIO/Trabajo{\_}fin{\_}asignatura/dpgmm.pdf:pdf},
isbn = {1000-9000},
issn = {10009000},
journal = {Journal of Computer Science and Technology},
keywords = {Bayesian nonparametrics,Dirichlet processes,Gaussian mixtures},
number = {4},
pages = {653--664},
title = {{Dirichlet process gaussian mixture models: Choice of the base distribution}},
volume = {25},
year = {2010}
}
@article{Lodhi2002,
abstract = {We propose a novel approach for categorizing text documents based on the use of a special kernel. The kernel is an inner product in the feature space generated by all subsequences of length k. A subsequence is any ordered sequence of k characters occurring in the text though not necessarily contiguously. The subsequences are weighted by an exponentially decaying factor of their full length in the text, hence emphasising those occurrences that are close to contiguous. A direct computation of this feature vector would involve a prohibitive amount of computation even for modest values of k, since the dimension of the feature space grows exponentially with k. The paper describes how despite this fact the inner product can be efficiently evaluated by a dynamic programming technique. Experimental comparisons of the performance of the kernel compared with a standard word feature space kernel (Joachims, 1998) show positive results on modestly sized datasets. The case of contiguous subsequences is also considered for comparison with the subsequences kernel with different decay factors. For larger documents and datasets the paper introduces an approximation technique that is shown to deliver good approximations efficiently for large datasets},
author = {Lodhi, H and Saunders, C and Shawe-Taylor, J and Cristianini, N and Watkins, C},
doi = {10.1162/153244302760200687},
file = {:home/hawk31/ownCloud/Master/FBIO/Trabajo{\_}fin{\_}asignatura/stringkernels.pdf:pdf},
isbn = {1532-4435},
issn = {0003-6951},
journal = {Journal of Machine Learning Research},
keywords = {String Kernels,text classification},
pages = {419--444},
title = {{Text Classification using String Kernels}},
url = {http://discovery.ucl.ac.uk/13443/},
volume = {2},
year = {2002}
}
